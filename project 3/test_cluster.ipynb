{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 31, using nperseg = 31\n",
      "  .format(nperseg, input_length))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate, fftpack, stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Module to interpolate values.\n",
    "def interpolate_missing_vals(i):\n",
    "    s1 = []\n",
    "    store_val = []\n",
    "    x = 0\n",
    "    for elems in if_meal_df.iloc[i]:\n",
    "        s1.append(elems)\n",
    "        \n",
    "    data = {'vals' : s1}\n",
    "    df = pd.DataFrame(data = data)\n",
    "\n",
    "    # Interpolate the values.\n",
    "    #df['vals'].interpolate(method = 'polynomial', order = 3, inplace = True)\n",
    "    df['vals'].interpolate(method = 'pad', limit = 2, inplace = True)\n",
    "    store_val = df['vals'].tolist()\n",
    "    \n",
    "    for cols in if_meal_df.columns:\n",
    "        if_meal_df.at[i, cols] = store_val[x]\n",
    "        x += 1\n",
    "\n",
    "# Module to perform polynomial fit\n",
    "# to get the coefficient values.\n",
    "def perform_polyfit(i):\n",
    "    colmns = ['coeff_0', 'coeff_1', 'coeff_2']\n",
    "    co_eff = []\n",
    "    itr = 0\n",
    "    vals = new_training.iloc[i].tolist()\n",
    "    interval = [j * 5 for j in range(0, len(new_training.iloc[i]))]\n",
    "    p_fit = np.polyfit(interval, vals, 2).tolist()\n",
    "    co_eff.extend(p_fit)\n",
    "    # Plot chart\n",
    "    # Uncomment the below lines to\n",
    "    # plot the curve.\n",
    "    #plt.plot(p_fit)\n",
    "    #plt.show()\n",
    "    for cols in colmns:\n",
    "        feature_df.at[i, cols] = co_eff[itr]\n",
    "        itr += 1\n",
    "\n",
    "# Module to perform fft.\n",
    "def perform_fft(i):\n",
    "    itr = 0\n",
    "    vals = new_training.iloc[i].tolist()\n",
    "    fft_plot = abs(fftpack.fft(vals))\n",
    "    fft_vals = sorted(set(fft_plot), reverse = True)\n",
    "    try:\n",
    "        feature_df.at[i, 'high_1'] = fft_vals[1]\n",
    "        feature_df.at[i, 'high_2'] = fft_vals[2]\n",
    "        feature_df.at[i, 'high_3'] = fft_vals[3]\n",
    "    except:\n",
    "        feature_df.at[i, 'high_1'] = 0\n",
    "        feature_df.at[i, 'high_2'] = 0\n",
    "        feature_df.at[i, 'high_3'] = 0\n",
    "    # Plot chart\n",
    "    # Uncomment the below lines to\n",
    "    # plot the curve.\n",
    "    #print('FFT', [fft_vals[1], fft_vals[2], fft_vals[2]])\n",
    "    #plt.plot(fft_vals[1:])\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform CGM velocity method.\n",
    "def cgm_velocity(i):\n",
    "    window_size = 3\n",
    "    time_line = 15\n",
    "    velocity = []\n",
    "    val_store = new_training.iloc[i]\n",
    "    for j in range(0, len(new_training.iloc[i]) - window_size):\n",
    "        interim = (val_store[j] - val_store[j + window_size]) / time_line\n",
    "        velocity.append(interim)\n",
    "    # Find standard deviation of the series.\n",
    "    s_dev = pd.Series(velocity).std()\n",
    "    mean_val = pd.Series(velocity).mean()\n",
    "    median_val = pd.Series(velocity).median()\n",
    "    feature_df.at[i, 'cgm_velocity_stdv'] = s_dev\n",
    "    feature_df.at[i, 'cgm_velocity_mean'] = mean_val\n",
    "    feature_df.at[i, 'cgm_velocity_median'] = median_val\n",
    "    #plt.plot(velocity)\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform Welch method.\n",
    "def perform_welch(i):\n",
    "    store_interim = new_training.iloc[i]\n",
    "    hz, welch_values  = np.array((signal.welch(store_interim)))\n",
    "    welch_std = pd.Series(welch_values).std()\n",
    "    welch_mean = pd.Series(welch_values).mean()\n",
    "    welch_median = pd.Series(welch_values).median()\n",
    "    feature_df.at[i, 'max_welch'] = max(welch_values)\n",
    "    feature_df.at[i, 'std_welch'] = welch_std\n",
    "    feature_df.at[i, 'mean_welch'] = welch_mean\n",
    "    feature_df.at[i, 'median_welch'] = welch_median\n",
    "    #plt.plot(hz, welch_values)\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform PCA.\n",
    "def performPCA():\n",
    "    pc_features = feature_df.columns\n",
    "    feature_matrix = feature_df.loc[:, pc_features].values\n",
    "    # Normalize the feature values.\n",
    "    feature_matrix = StandardScaler().fit_transform(feature_matrix)\n",
    "    pca_cons = PCA(n_components = 10)\n",
    "    principal_components = pca_cons.fit_transform(feature_matrix)\n",
    "    final_component = pd.DataFrame(data = principal_components, \n",
    "                                   columns = ['component_1', 'component_2', \n",
    "                                              'component_3', 'component_4', \n",
    "                                              'component_5', 'component_6', \n",
    "                                              'component_7', 'component_8', \n",
    "                                              'component_9', 'component_10'])\n",
    "    pca_var = pca_cons.explained_variance_ratio_\n",
    "    pc_comps = (abs(pca_cons.components_))\n",
    "    #print(abs(pca_cons.components_))\n",
    "    pca_var = ['{:f}'.format(item) for item in pca_var]\n",
    "    #print(pca_var)\n",
    "    return final_component\n",
    "\n",
    "def add_bins(loc_df):\n",
    "    # Add bins\n",
    "    for k in range(0, len(loc_df)):\n",
    "        if loc_df['true_amt'][k] == 0:\n",
    "            loc_df['bins'][k] = 'c_0' \n",
    "        elif loc_df['true_amt'][k] > 0 and loc_df['true_amt'][k] <= 20:\n",
    "            loc_df['bins'][k] = 'c_0_20'\n",
    "        elif loc_df['true_amt'][k] > 20 and loc_df['true_amt'][k] <= 40:\n",
    "            loc_df['bins'][k] = 'c_20_40'\n",
    "        elif loc_df['true_amt'][k] > 40 and loc_df['true_amt'][k] <= 60:\n",
    "            loc_df['bins'][k] = 'c_40_60' \n",
    "        elif loc_df['true_amt'][k] > 60 and loc_df['true_amt'][k] <= 80:\n",
    "            loc_df['bins'][k] = 'c_60_80'\n",
    "        else:\n",
    "            loc_df['bins'][k] = 'c_80_100'\n",
    "    return loc_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Module to read all the csv files\n",
    "    col_names = []\n",
    "    for i in range(1, 32):\n",
    "        col_name_val = 'c' + str(i)\n",
    "        col_names.append(col_name_val)\n",
    "    \n",
    "    # Read the meal amount data.\n",
    "    meal_amt1 = pd.read_csv('Dataset/mealAmountData1.csv', names = ['Meal_Amount'])\n",
    "    \n",
    "    # Read the files\n",
    "    list_files = os.listdir('test_files')\n",
    "    read_name = 'test_files/' + list_files[0]\n",
    "    if_meal_df = pd.read_csv(read_name, names = col_names)\n",
    "    for i in range(1, len(list_files)):\n",
    "        read_name = 'test_files/' + list_files[i]\n",
    "        test2 = pd.read_csv(read_name, names = col_names)\n",
    "        if_meal_df = pd.concat([if_meal_df, test2], ignore_index = True)\n",
    "    if_meal_df['amount'] = meal_amt1[0 : 51]\n",
    "    # Create the feature data frame.\n",
    "    feature_df = pd.DataFrame(columns = ['coeff_0', 'coeff_1', 'coeff_2', 'high_1', 'high_2', 'high_3', 'cgm_velocity_stdv', 'cgm_velocity_mean', 'cgm_velocity_median', 'max_welch', 'std_welch', 'mean_welch', 'median_welch'])\n",
    "    # Interpolate the missing values in\n",
    "    # meal data.\n",
    "    for i in range(len(if_meal_df)):\n",
    "        interpolate_missing_vals(i)\n",
    "    \n",
    "    # Remove all NA values from the dataframe\n",
    "    if_meal_df = if_meal_df.dropna()\n",
    "    \n",
    "    # Create the training dataframe\n",
    "    traning_interim_df = if_meal_df.copy()\n",
    "\n",
    "    # Add Features\n",
    "    new_training = traning_interim_df.loc[: , 'c1' : 'c31'].copy()\n",
    "    # Perform Polyfit\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_polyfit(i)\n",
    "\n",
    "    # Perform polyfit feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_fft(i)\n",
    "    \n",
    "    # Perform CGM velocity feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        cgm_velocity(i)\n",
    "    \n",
    "    # Perform welch feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_welch(i)\n",
    "    \n",
    "    # Perform PCA\n",
    "    final_df_km = performPCA()\n",
    "    final_df_db = performPCA()\n",
    "    \n",
    "    loaded_model_km = pickle.load(open('pkl_files/kmean_fit.pkl', 'rb'))\n",
    "    result_km = loaded_model_km.predict(final_df_km)\n",
    "    \n",
    "    loaded_model_db = pickle.load(open('pkl_files/dbscan_fit.pkl', 'rb'))\n",
    "    result_db = loaded_model_db.predict(final_df_db)\n",
    "    \n",
    "    for i in range(0, len(result_db)):\n",
    "        if result_db[i] == 'c_0':\n",
    "            result_db[i] = 1\n",
    "        elif result_db[i] == 'c_0_20':\n",
    "            result_db[i] = 2\n",
    "        elif result_db[i] == 'c_20_40':\n",
    "            result_db[i] = 3\n",
    "        elif result_db[i] == 'c_40_60':\n",
    "            result_db[i] = 4\n",
    "        elif result_db[i] == 'c_60_80':\n",
    "            result_db[i] = 5\n",
    "        else:\n",
    "            result_db[i] = 6\n",
    "\n",
    "    for i in range(0, len(result_km)):\n",
    "        if result_km[i] == 'c_0':\n",
    "            result_km[i] = 1\n",
    "        elif result_km[i] == 'c_0_20':\n",
    "            result_km[i] = 2\n",
    "        elif result_km[i] == 'c_20_40':\n",
    "            result_km[i] = 3\n",
    "        elif result_km[i] == 'c_40_60':\n",
    "            result_km[i] = 4\n",
    "        elif result_km[i] == 'c_60_80':\n",
    "            result_km[i] = 5\n",
    "        else:\n",
    "            result_km[i] = 6\n",
    "    \n",
    "    # Create a csv file\n",
    "    df_op = pd.DataFrame(columns = ['dbscan', 'kmeans'])\n",
    "    df_op['dbscan'] = result_db\n",
    "    df_op['kmeans'] = result_km\n",
    "    df_op.to_csv('result_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
