{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:221: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 31, using nperseg = 31\n",
      "  .format(nperseg, input_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree\n",
      "Min : 0.4090909090909091\n",
      "Mean : 0.5643939393939393\n",
      "Max : 0.7727272727272727\n",
      "------------\n",
      "Random forest\n",
      "Max- 0.75\n",
      "Mean- 0.6301515151515151\n",
      "Min- 0.5111111111111111\n",
      "------------\n",
      "SVM\n",
      "Max- 0.7045454545454546\n",
      "Mean- 0.6301010101010102\n",
      "Min- 0.5333333333333333\n",
      "------------\n",
      "Logistic regression\n",
      "Max- 0.7272727272727273\n",
      "Mean- 0.6345959595959596\n",
      "Min- 0.5333333333333333\n",
      "------------\n",
      "mlp\n",
      "Max- 0.7272727272727273\n",
      "Mean- 0.6388383838383839\n",
      "Min- 0.5333333333333333\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate, fftpack, stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Module to interpolate values.\n",
    "def interpolate_missing_vals(i, identifier):\n",
    "    s1 = []\n",
    "    store_val = []\n",
    "    x = 0\n",
    "    if identifier == 'meal':\n",
    "        for elems in meal_df.iloc[i]:\n",
    "            s1.append(elems)\n",
    "    else:\n",
    "        for elems in no_meal_df.iloc[i]:\n",
    "            s1.append(elems)        \n",
    "    \n",
    "    data = {'vals' : s1}\n",
    "    df = pd.DataFrame(data = data)\n",
    "\n",
    "    # Interpolate the values.\n",
    "    #df['vals'].interpolate(method = 'polynomial', order = 3, inplace = True)\n",
    "    df['vals'].interpolate(method = 'pad', limit = 2, inplace = True)\n",
    "    store_val = list(df['vals'])\n",
    "    \n",
    "    if identifier == 'meal':\n",
    "        for cols in meal_df.columns:\n",
    "            meal_df.at[i, cols] = store_val[x]\n",
    "            x += 1\n",
    "    else:\n",
    "        for cols in no_meal_df.columns:\n",
    "            no_meal_df.at[i, cols] = store_val[x]\n",
    "            x += 1\n",
    "\n",
    "# Module to perform polynomial fit\n",
    "# to get the coefficient values.\n",
    "def perform_polyfit(i):\n",
    "    colmns = ['coeff_0', 'coeff_1', 'coeff_2']\n",
    "    co_eff = []\n",
    "    itr = 0\n",
    "    vals = list(new_training.iloc[i])\n",
    "    interval = [j * 5 for j in range(0, len(new_training.iloc[i]))]\n",
    "    p_fit = list(np.polyfit(interval, vals, 2))\n",
    "    co_eff.extend(p_fit)\n",
    "    # Plot chart\n",
    "    # Uncomment the below lines to\n",
    "    # plot the curve.\n",
    "    #plt.plot(p_fit)\n",
    "    #plt.show()\n",
    "    for cols in colmns:\n",
    "        feature_df.at[i, cols] = co_eff[itr]\n",
    "        itr += 1\n",
    "\n",
    "# Module to perform fft.\n",
    "def perform_fft(i):\n",
    "    itr = 0\n",
    "    vals = list(new_training.iloc[i])\n",
    "    fft_plot = abs(fftpack.fft(vals))\n",
    "    fft_vals = sorted(set(fft_plot), reverse = True)\n",
    "    feature_df.at[i, 'high_1'] = fft_vals[1]\n",
    "    feature_df.at[i, 'high_2'] = fft_vals[2]\n",
    "    feature_df.at[i, 'high_3'] = fft_vals[3]\n",
    "    # Plot chart\n",
    "    # Uncomment the below lines to\n",
    "    # plot the curve.\n",
    "    #print('FFT', [fft_vals[1], fft_vals[2], fft_vals[2]])\n",
    "    #plt.plot(fft_vals[1:])\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform CGM velocity method.\n",
    "def cgm_velocity(i):\n",
    "    window_size = 3\n",
    "    time_line = 15\n",
    "    velocity = []\n",
    "    val_store = new_training.iloc[i]\n",
    "    for j in range(0, len(new_training.iloc[i]) - window_size):\n",
    "        interim = (val_store[j] - val_store[j + window_size]) / time_line\n",
    "        velocity.append(interim)\n",
    "    # Find standard deviation of the series.\n",
    "    s_dev = pd.Series(velocity).std()\n",
    "    mean_val = pd.Series(velocity).mean()\n",
    "    median_val = pd.Series(velocity).median()\n",
    "    feature_df.at[i, 'cgm_velocity_stdv'] = s_dev\n",
    "    feature_df.at[i, 'cgm_velocity_mean'] = mean_val\n",
    "    feature_df.at[i, 'cgm_velocity_median'] = median_val\n",
    "    #plt.plot(velocity)\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform Welch method.\n",
    "def perform_welch(i):\n",
    "    store_interim = new_training.iloc[i]\n",
    "    hz, welch_values  = np.array((signal.welch(store_interim)))\n",
    "    welch_std = pd.Series(welch_values).std()\n",
    "    welch_mean = pd.Series(welch_values).mean()\n",
    "    welch_median = pd.Series(welch_values).median()\n",
    "    feature_df.at[i, 'max_welch'] = max(welch_values)\n",
    "    feature_df.at[i, 'std_welch'] = welch_std\n",
    "    feature_df.at[i, 'mean_welch'] = welch_mean\n",
    "    feature_df.at[i, 'median_welch'] = welch_median\n",
    "    #plt.plot(hz, welch_values)\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform PCA.\n",
    "def performPCA():\n",
    "    pc_features = feature_df.columns\n",
    "    feature_matrix = feature_df.loc[:, pc_features].values\n",
    "    # Normalize the feature values.\n",
    "    feature_matrix = StandardScaler().fit_transform(feature_matrix)\n",
    "    pca_cons = PCA(n_components = 10)\n",
    "    principal_components = pca_cons.fit_transform(feature_matrix)\n",
    "    final_component = pd.DataFrame(data = principal_components, \n",
    "                                   columns = ['component_1', 'component_2', \n",
    "                                              'component_3', 'component_4', \n",
    "                                              'component_5', 'component_6', \n",
    "                                              'component_7', 'component_8', \n",
    "                                              'component_9', 'component_10'])\n",
    "    pca_var = pca_cons.explained_variance_ratio_\n",
    "    pc_comps = (abs(pca_cons.components_))\n",
    "    #print(abs(pca_cons.components_))\n",
    "    pca_var = ['{:f}'.format(item) for item in pca_var]\n",
    "    #print(pca_var)\n",
    "    return final_component\n",
    "\n",
    "def modelDecisionTree(X,y):\n",
    "    scores = []\n",
    "    accuracy = []\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[\n",
    "            test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted = model.predict(X_test)\n",
    "        accuracy.append(metrics.accuracy_score(y_test, predicted))\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "\n",
    "    print(\"DecisionTree\")\n",
    "    print(\"Max- \" + str(np.min(accuracy)))\n",
    "    print(\"Mean- \"+ str(np.mean(accuracy)))\n",
    "    print(\"Min- \"+ str( np.max(accuracy)))\n",
    "\n",
    "    return model\n",
    "\n",
    "def execute_classifiers(model, X_train, X_test, Y_train, Y_test):\n",
    "    model.fit(X_train,Y_train)\n",
    "    score = model.score(X_test, Y_test)\n",
    "    predict = model.predict(X_test)\n",
    "    accuracy_score = metrics.accuracy_score(Y_test, predict)\n",
    "    return accuracy_score\n",
    "\n",
    "def execute_fit(model, X_train, X_test, Y_train, Y_test):\n",
    "    model.fit(X_train,Y_train)\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Store accuracy scores\n",
    "    rforest = []\n",
    "    svc = []\n",
    "    lregrr = []\n",
    "    mlp = []\n",
    "    check_max = {}\n",
    "    checker = 0\n",
    "    algos = ''\n",
    "    \n",
    "    # Module to read all the csv files\n",
    "    col_names = []\n",
    "    for i in range(1, 32):\n",
    "        col_name_val = 'c' + str(i)\n",
    "        col_names.append(col_name_val)\n",
    "    #col_names.append('class')\n",
    "    # Read the meal data\n",
    "    meal1 = pd.read_csv('Resources/MealNoMealData/mealData1.csv', names = col_names)\n",
    "    meal2 = pd.read_csv('Resources/MealNoMealData/mealData2.csv', names = col_names)\n",
    "    meal3 = pd.read_csv('Resources/MealNoMealData/mealData3.csv', names = col_names)\n",
    "    meal4 = pd.read_csv('Resources/MealNoMealData/mealData4.csv', names = col_names)\n",
    "    meal5 = pd.read_csv('Resources/MealNoMealData/mealData5.csv', names = col_names)\n",
    "    meal_df = pd.concat([meal1, meal2, meal3, meal4, meal5], ignore_index = True)\n",
    "\n",
    "    # Read no meal data\n",
    "    no_meal1 = pd.read_csv('Resources/MealNoMealData/Nomeal1.csv', names = col_names)\n",
    "    no_meal2 = pd.read_csv('Resources/MealNoMealData/Nomeal2.csv', names = col_names)\n",
    "    no_meal3 = pd.read_csv('Resources/MealNoMealData/Nomeal3.csv', names = col_names)\n",
    "    no_meal4 = pd.read_csv('Resources/MealNoMealData/Nomeal4.csv', names = col_names)\n",
    "    no_meal5 = pd.read_csv('Resources/MealNoMealData/Nomeal5.csv', names = col_names)\n",
    "    no_meal_df = pd.concat([no_meal1, no_meal2, no_meal3, no_meal4, no_meal5], ignore_index = True)\n",
    "    \n",
    "    # Create the feature data frame.\n",
    "    feature_df = pd.DataFrame(columns = ['coeff_0', 'coeff_1', 'coeff_2', 'high_1', 'high_2', 'high_3', 'cgm_velocity_stdv', 'cgm_velocity_mean', 'cgm_velocity_median', 'max_welch', 'std_welch', 'mean_welch', 'median_welch'])\n",
    "    # Interpolate the missing values in\n",
    "    # meal data.\n",
    "    for i in range(len(meal_df)):\n",
    "        interpolate_missing_vals(i, 'meal')\n",
    "    \n",
    "    # Interpolate the missing values in\n",
    "    # no meal data.\n",
    "    for i in range(len(no_meal_df)):\n",
    "        interpolate_missing_vals(i, 'no_meal')\n",
    "\n",
    "    # Remove all NA values from the dataframe\n",
    "    meal_df = meal_df.dropna()\n",
    "    no_meal_df = no_meal_df.dropna()\n",
    "\n",
    "    # Add classs\n",
    "    meal_df['class'] = 1\n",
    "    no_meal_df['calss'] = 0\n",
    "\n",
    "    # Create the training dataframe\n",
    "    traning_interim_df = pd.concat([meal_df, no_meal_df])\n",
    "    # Add classs \n",
    "    meal_df['class'] = '1'\n",
    "    no_meal_df['class'] = '0'\n",
    "\n",
    "    # Create the training dataframe\n",
    "    traning_interim_df = pd.concat([meal_df, no_meal_df], ignore_index = True, sort = False)\n",
    "    \n",
    "    # Add Features\n",
    "    new_training = traning_interim_df.loc[: , 'c1' : 'c31'].copy()\n",
    "\n",
    "    # Extract class labels\n",
    "    class_labels['class'] = traning_interim_df['class'].copy()\n",
    "\n",
    "    # Perform Polyfit\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_polyfit(i)\n",
    "\n",
    "    # Perform polyfit feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_fft(i)\n",
    "    \n",
    "    # Perform CGM velocity feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        cgm_velocity(i)\n",
    "    \n",
    "    # Perform welch feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_welch(i)\n",
    "    \n",
    "    # Perform PCA\n",
    "    final_df = performPCA()\n",
    "\n",
    "    # Perform k fold\n",
    "    k_cross_ret = KFold(n_splits = 10,random_state = 42, shuffle = True)\n",
    "    for train_index, test_index in k_cross_ret.split(p_df):\n",
    "        train_feature, test_feature, train_label, test_label = p_df.iloc[train_index],p_df.iloc[test_index],\\\n",
    "                                       class_labels['class'].iloc[train_index],class_labels['class'].iloc[test_index]\n",
    "    \n",
    "        rforest.append(execute_classifiers(RandomForestClassifier(n_estimators = 550), train_feature, test_feature, train_label, test_label))\n",
    "    \n",
    "        svc.append(execute_classifiers(SVC(gamma='scale'),train_feature, test_feature, train_label, test_label))\n",
    "    \n",
    "        lregrr.append(execute_classifiers(LogisticRegression(solver='lbfgs'),train_feature, test_feature, train_label, test_label))\n",
    "    \n",
    "        mlp.append(execute_classifiers(MLPClassifier(hidden_layer_sizes=(300), max_iter=5000,activation = 'relu',solver='adam',random_state=1)\\\n",
    "                             ,train_feature, test_feature, train_label, test_label))\n",
    "    \n",
    "    x = modelDecisionTree(p_df, class_labels)\n",
    "    print('------------')\n",
    "    check_max['random_forrest'] = statistics.mean(rforest)\n",
    "    print('Random forest')\n",
    "    print('Max-', max(rforest))\n",
    "    print('Mean-', statistics.mean(rforest))\n",
    "    print('Min-', min(rforest))\n",
    "    print('------------')\n",
    "    check_max['svm'] = statistics.mean(svc)\n",
    "    print('SVM')\n",
    "    print('Max-', max(svc))\n",
    "    print('Mean-', statistics.mean(svc))\n",
    "    print('Min-', min(svc))\n",
    "    print('------------')\n",
    "    check_max['logistic'] = statistics.mean(lregrr)\n",
    "    print('Logistic regression')\n",
    "    print('Max-', max(lregrr))\n",
    "    print('Mean-', statistics.mean(lregrr))\n",
    "    print('Min-', min(lregrr))\n",
    "    print('------------')\n",
    "    check_max['mlp'] = statistics.mean(mlp)\n",
    "    print('mlp')\n",
    "    print('Max-', max(mlp))\n",
    "    print('Mean-', statistics.mean(mlp))\n",
    "    print('Min-', min(mlp))\n",
    "    print('------------')\n",
    "    # Find algo with best accuracy and\n",
    "    # fit the model.\n",
    "    filename = 'trained.pkl'\n",
    "    for a, b in check_max.items():\n",
    "        if b > checker:\n",
    "            algos = a\n",
    "            checker = b\n",
    "    if algos == 'random_forrest':\n",
    "        m_fit = execute_fit(RandomForestClassifier(n_estimators = 550), train_feature, test_feature, train_label, test_label)\n",
    "        pickle.dump(m_fit, open(filename, 'wb'))\n",
    "\n",
    "    elif algos == 'svm':\n",
    "        m_fit = execute_fit(SVC(gamma='scale'),train_feature, test_feature, train_label, test_label)\n",
    "        pickle.dump(m_fit, open(filename, 'wb'))\n",
    "    \n",
    "    elif algos == 'logistic':\n",
    "        m_fit = execute_fit(LogisticRegression(solver='lbfgs'),train_feature, test_feature, train_label, test_label)\n",
    "        pickle.dump(m_fit, open(filename, 'wb'))\n",
    "\n",
    "    else:\n",
    "        m_fit = execute_fit(MLPClassifier(hidden_layer_sizes=(300), max_iter=5000,activation = 'relu',solver='adam',random_state=1)\\\n",
    "                             ,train_feature, test_feature, train_label, test_label)\n",
    "        pickle.dump(m_fit, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
