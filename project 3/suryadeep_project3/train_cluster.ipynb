{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 31, using nperseg = 31\n",
      "  .format(nperseg, input_length))\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:225: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:227: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:221: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:219: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:217: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove bin c_0\n",
      "remove key 1\n",
      "Remove bin c_40_60\n",
      "remove key 5\n",
      "Remove bin c_0_20\n",
      "remove key 2\n",
      "Remove bin c_20_40\n",
      "remove key 0\n",
      "Remove bin c_60_80\n",
      "remove key 3\n",
      "Remove bin c_60_80\n",
      "remove key 3\n",
      "Remove bin c_0\n",
      "remove key 1\n",
      "Remove bin c_40_60\n",
      "remove key 5\n",
      "Remove bin c_0_20\n",
      "remove key 3\n",
      "Remove bin c_20_40\n",
      "remove key 0\n",
      "Remove bin c_20_40\n",
      "remove key 0\n",
      "Remove bin c_20_40\n",
      "remove key 0\n",
      "     component_1  component_2  component_3  component_4  component_5  \\\n",
      "0      -1.130629     1.465219    -1.717753     0.212919     0.307810   \n",
      "1      -0.702367     3.167100    -0.616872    -0.592263    -0.260659   \n",
      "2       2.036420    -0.907812    -0.339926     0.463290    -0.997103   \n",
      "3      -0.030501    -2.146047     0.349644     1.084255    -0.559875   \n",
      "4      -1.282547    -1.276291    -0.241693    -0.289298    -0.557769   \n",
      "..           ...          ...          ...          ...          ...   \n",
      "216    -2.950341    -0.656286     0.389397     0.600402    -0.452074   \n",
      "217     0.942294     0.543620     2.477093     0.025139    -0.782003   \n",
      "218     1.181601     2.731543     2.625778    -1.819785    -0.061720   \n",
      "219     4.853534    -2.488630    -0.072793    -1.017622    -0.930899   \n",
      "220    -1.401619     0.348907     0.002480    -0.477588     0.746427   \n",
      "\n",
      "     component_6  component_7  component_8  component_9  component_10  clust  \\\n",
      "0      -0.480038     0.713176     0.041974    -0.099288     -0.048585      5   \n",
      "1       0.143735    -0.722102    -0.500655    -0.094451      0.218778      5   \n",
      "2       0.030120    -0.042867     0.074209    -0.208534     -0.408687      3   \n",
      "3       0.311005     0.281327    -0.207243    -0.115144     -0.002336      3   \n",
      "4      -0.690302    -0.591132     0.261710    -0.028423     -0.222422      1   \n",
      "..           ...          ...          ...          ...           ...    ...   \n",
      "216    -0.538113    -0.068757     0.023322     0.459374      0.048517      1   \n",
      "217     0.982537     0.425042     0.776685    -0.226701      0.456352      0   \n",
      "218    -0.126093     0.360901     0.552375    -0.177956      0.211915      0   \n",
      "219    -0.820147     0.030905     0.147484     0.021388      0.081733      2   \n",
      "220     1.085609     0.910255    -0.275868     0.433916      0.036117      1   \n",
      "\n",
      "     true_amt     bins new_bins  \n",
      "0          45  c_40_60  c_40_60  \n",
      "1          65  c_60_80  c_40_60  \n",
      "2          60  c_40_60   c_0_20  \n",
      "3          80  c_60_80   c_0_20  \n",
      "4          45  c_40_60      c_0  \n",
      "..        ...      ...      ...  \n",
      "216        52  c_40_60      c_0  \n",
      "217        42  c_40_60  c_20_40  \n",
      "218        45  c_40_60  c_20_40  \n",
      "219         0      c_0  c_60_80  \n",
      "220        52  c_40_60      c_0  \n",
      "\n",
      "[221 rows x 14 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate, fftpack, stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Module to interpolate values.\n",
    "def interpolate_missing_vals(i, identifier):\n",
    "    s1 = []\n",
    "    store_val = []\n",
    "    x = 0\n",
    "    if identifier == 'meal':\n",
    "        for elems in meal_df.iloc[i]:\n",
    "            s1.append(elems)\n",
    "    else:\n",
    "        for elems in no_meal_df.iloc[i]:\n",
    "            s1.append(elems)        \n",
    "    \n",
    "    data = {'vals' : s1}\n",
    "    df = pd.DataFrame(data = data)\n",
    "\n",
    "    # Interpolate the values.\n",
    "    #df['vals'].interpolate(method = 'polynomial', order = 3, inplace = True)\n",
    "    df['vals'].interpolate(method = 'pad', limit = 2, inplace = True)\n",
    "    store_val = list(df['vals'])\n",
    "    \n",
    "    if identifier == 'meal':\n",
    "        for cols in meal_df.columns:\n",
    "            meal_df.at[i, cols] = store_val[x]\n",
    "            x += 1\n",
    "    else:\n",
    "        for cols in no_meal_df.columns:\n",
    "            no_meal_df.at[i, cols] = store_val[x]\n",
    "            x += 1\n",
    "\n",
    "# Module to perform polynomial fit\n",
    "# to get the coefficient values.\n",
    "def perform_polyfit(i):\n",
    "    colmns = ['coeff_0', 'coeff_1', 'coeff_2']\n",
    "    co_eff = []\n",
    "    itr = 0\n",
    "    vals = list(new_training.iloc[i])\n",
    "    interval = [j * 5 for j in range(0, len(new_training.iloc[i]))]\n",
    "    p_fit = list(np.polyfit(interval, vals, 2))\n",
    "    co_eff.extend(p_fit)\n",
    "    # Plot chart\n",
    "    # Uncomment the below lines to\n",
    "    # plot the curve.\n",
    "    #plt.plot(p_fit)\n",
    "    #plt.show()\n",
    "    for cols in colmns:\n",
    "        feature_df.at[i, cols] = co_eff[itr]\n",
    "        itr += 1\n",
    "\n",
    "# Module to perform fft.\n",
    "def perform_fft(i):\n",
    "    itr = 0\n",
    "    vals = list(new_training.iloc[i])\n",
    "    fft_plot = abs(fftpack.fft(vals))\n",
    "    fft_vals = sorted(set(fft_plot), reverse = True)\n",
    "    feature_df.at[i, 'high_1'] = fft_vals[1]\n",
    "    feature_df.at[i, 'high_2'] = fft_vals[2]\n",
    "    feature_df.at[i, 'high_3'] = fft_vals[3]\n",
    "    # Plot chart\n",
    "    # Uncomment the below lines to\n",
    "    # plot the curve.\n",
    "    #print('FFT', [fft_vals[1], fft_vals[2], fft_vals[2]])\n",
    "    #plt.plot(fft_vals[1:])\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform CGM velocity method.\n",
    "def cgm_velocity(i):\n",
    "    window_size = 3\n",
    "    time_line = 15\n",
    "    velocity = []\n",
    "    val_store = new_training.iloc[i]\n",
    "    for j in range(0, len(new_training.iloc[i]) - window_size):\n",
    "        interim = (val_store[j] - val_store[j + window_size]) / time_line\n",
    "        velocity.append(interim)\n",
    "    # Find standard deviation of the series.\n",
    "    s_dev = pd.Series(velocity).std()\n",
    "    mean_val = pd.Series(velocity).mean()\n",
    "    median_val = pd.Series(velocity).median()\n",
    "    feature_df.at[i, 'cgm_velocity_stdv'] = s_dev\n",
    "    feature_df.at[i, 'cgm_velocity_mean'] = mean_val\n",
    "    feature_df.at[i, 'cgm_velocity_median'] = median_val\n",
    "    #plt.plot(velocity)\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform Welch method.\n",
    "def perform_welch(i):\n",
    "    store_interim = new_training.iloc[i]\n",
    "    hz, welch_values  = np.array((signal.welch(store_interim)))\n",
    "    welch_std = pd.Series(welch_values).std()\n",
    "    welch_mean = pd.Series(welch_values).mean()\n",
    "    welch_median = pd.Series(welch_values).median()\n",
    "    feature_df.at[i, 'max_welch'] = max(welch_values)\n",
    "    feature_df.at[i, 'std_welch'] = welch_std\n",
    "    feature_df.at[i, 'mean_welch'] = welch_mean\n",
    "    feature_df.at[i, 'median_welch'] = welch_median\n",
    "    #plt.plot(hz, welch_values)\n",
    "    #plt.show()\n",
    "\n",
    "# Module to perform PCA.\n",
    "def performPCA():\n",
    "    pc_features = feature_df.columns\n",
    "    feature_matrix = feature_df.loc[:, pc_features].values\n",
    "    # Normalize the feature values.\n",
    "    feature_matrix = StandardScaler().fit_transform(feature_matrix)\n",
    "    pca_cons = PCA(n_components = 10)\n",
    "    principal_components = pca_cons.fit_transform(feature_matrix)\n",
    "    final_component = pd.DataFrame(data = principal_components, \n",
    "                                   columns = ['component_1', 'component_2', \n",
    "                                              'component_3', 'component_4', \n",
    "                                              'component_5', 'component_6', \n",
    "                                              'component_7', 'component_8', \n",
    "                                              'component_9', 'component_10'])\n",
    "    pca_var = pca_cons.explained_variance_ratio_\n",
    "    pc_comps = (abs(pca_cons.components_))\n",
    "    #print(abs(pca_cons.components_))\n",
    "    pca_var = ['{:f}'.format(item) for item in pca_var]\n",
    "    #print(pca_var)\n",
    "    return final_component    \n",
    "    \n",
    "def db_scan():\n",
    "    m = DBSCAN(eps = 1.9, min_samples = 4,algorithm = 'ball_tree', metric='minkowski', leaf_size=90, p=2)\n",
    "    m.fit(final_df)\n",
    "    clusters = m.labels_\n",
    "    #colors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']\n",
    "    #vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n",
    "    #plt.scatter(final_df.loc[:,'component_1'], final_df.loc[:,'component_2'], c=vectorizer(clusters))\n",
    "    # Add the cluster columns.\n",
    "    db_scan_df['clust'] = clusters\n",
    "    \n",
    "    #print(db_scan_df.groupby(['clust']).count())\n",
    "    \n",
    "    # Get the number of cluster from\n",
    "    # dbscan.\n",
    "    n_clusts = len(np.unique(clusters))\n",
    "    un_clusts = np.unique(clusters)\n",
    "    \n",
    "    # Get the count of elements in each\n",
    "    # cluster\n",
    "    a = db_scan_df.groupby(['clust']).count()\n",
    "    \n",
    "    # Ignore the outlier value.\n",
    "    store_counts = a.loc[0:,'component_1']\n",
    "\n",
    "    # Store the cluster ids with maximum\n",
    "    # density.\n",
    "    re_clusts = []\n",
    "    re_clusts.append(-1)\n",
    "    mx_val = max(store_counts)\n",
    "    for i in range(0, len(store_counts)):\n",
    "        if store_counts[i] == mx_val:\n",
    "            re_clusts.append(i)\n",
    "\n",
    "    # Create new dataframe with max val\n",
    "    # cluster.\n",
    "    max_clust_df = pd.DataFrame(columns = db_scan_df.columns)\n",
    "    for i in range(0, len(db_scan_df)):\n",
    "        # Extract the clusters with maximum frequency.\n",
    "        if db_scan_df['clust'][i] in re_clusts:\n",
    "            max_clust_df.loc[i] = db_scan_df.loc[i]\n",
    "\n",
    "    # K in kmeans based on clusters\n",
    "    # found in dbscan\n",
    "    rem  = n_clusts - len(re_clusts)\n",
    "    k_mns = 6 - rem\n",
    "    itr = 6 - rem\n",
    "\n",
    "    # change cluster names.\n",
    "    good_clusts = list(set(un_clusts) ^ set (re_clusts))\n",
    "    for clts in good_clusts:\n",
    "        for i in range(0, len(db_scan_df['clust'])):\n",
    "            if db_scan_df['clust'][i] == clts:\n",
    "                db_scan_df['clust'][i] = itr\n",
    "        itr += 1\n",
    "\n",
    "    # Perform K means.\n",
    "    kmeans = KMeans(n_clusters = k_mns, init = 'k-means++', max_iter = 1000, n_init = 10, random_state = 0)\n",
    "    pred_y = kmeans.fit_predict(max_clust_df)\n",
    "\n",
    "    # Update the db scan df after\n",
    "    # kmeans.\n",
    "    max_clust_df['clust'] = pred_y\n",
    "    for idxs in max_clust_df.index:\n",
    "        db_scan_df.loc[idxs] = max_clust_df.loc[idxs, 'component_1' : 'clust']\n",
    "        \n",
    "def perform_kmns():\n",
    "    # Perform K means.\n",
    "    kmeans = KMeans(n_clusters = 6, init = 'k-means++', max_iter = 1000, n_init = 10, random_state = 0)\n",
    "    k_mns_pred = kmeans.fit_predict(k_means_df)\n",
    "    k_means_df['clust'] = k_mns_pred\n",
    "\n",
    "def add_bins(loc_df):\n",
    "    # Add bins\n",
    "    for k in range(0, len(loc_df)):\n",
    "        if loc_df['true_amt'][k] == 0:\n",
    "            loc_df['bins'][k] = 'c_0' \n",
    "        elif loc_df['true_amt'][k] > 0 and loc_df['true_amt'][k] <= 20:\n",
    "            loc_df['bins'][k] = 'c_0_20'\n",
    "        elif loc_df['true_amt'][k] > 20 and loc_df['true_amt'][k] <= 40:\n",
    "            loc_df['bins'][k] = 'c_20_40'\n",
    "        elif loc_df['true_amt'][k] > 40 and loc_df['true_amt'][k] <= 60:\n",
    "            loc_df['bins'][k] = 'c_40_60' \n",
    "        elif loc_df['true_amt'][k] > 60 and loc_df['true_amt'][k] <= 80:\n",
    "            loc_df['bins'][k] = 'c_60_80'\n",
    "        else:\n",
    "            loc_df['bins'][k] = 'c_80_100'\n",
    "    return loc_df\n",
    "\n",
    "def add_new_truth(new_loc_df):\n",
    "    # Find the grouped counts.\n",
    "    d_interim = new_loc_df.groupby(['clust', 'bins']).count()\n",
    "\n",
    "    storage_dict = defaultdict(list)\n",
    "    itr = 0\n",
    "\n",
    "    # Create the dictionary to store\n",
    "    # the counts and clusters.\n",
    "    while itr <= 6 - 1:\n",
    "        for i in range(0, len(d_interim['component_1'][itr].index)):\n",
    "            t = (d_interim['component_1'][itr].index[i], d_interim['component_1'][itr][i])\n",
    "            storage_dict[itr].append(t)\n",
    "        itr += 1\n",
    "\n",
    "    truth_bins = ['c_0', 'c_0_20', 'c_20_40', 'c_40_60', 'c_60_80', 'c_80_100']\n",
    "    truth_clusts = [0, 1, 2, 3, 4, 5] \n",
    "    count_vals = list(d_interim['component_1'])\n",
    "\n",
    "    # Form the id list.\n",
    "    idx_lst = []\n",
    "    idx_cust = []\n",
    "    for elems in list(d_interim['component_1'].index):\n",
    "        idx_lst.append(elems[1])\n",
    "        idx_cust.append(elems[0])\n",
    "\n",
    "    assigned_bin = {}\n",
    "    del_bin = 'na'\n",
    "    del_clust = 9999\n",
    "    iter_loop = 1\n",
    "\n",
    "    # Iterate for all the clusters\n",
    "    while iter_loop <= 6:\n",
    "        flag = 0\n",
    "        mx_count_val = int(max(count_vals))\n",
    "    \n",
    "        for key, vals in storage_dict.items():\n",
    "            for a, b in vals:\n",
    "                # Assign cluster to bin.\n",
    "                if b == mx_count_val:\n",
    "                    del_bin = a\n",
    "                    truth_bins.remove(del_bin)\n",
    "                \n",
    "                    del_clust = key\n",
    "                    truth_clusts.remove(del_clust)\n",
    "                \n",
    "                    assigned_bin[a] = key\n",
    "                    # Remove the assigned cluster\n",
    "                    # from the storage dict.\n",
    "                    storage_dict.pop(key)\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                break\n",
    "            \n",
    "        # Remove the popped bin\n",
    "        print('Remove bin', del_bin)\n",
    "        #truth_bins.remove(del_bin)\n",
    "        print('remove key', del_clust)\n",
    "        #truth_clusts.remove(del_clust)\n",
    "    \n",
    "        for key, vals in storage_dict.items():\n",
    "            for a, b in vals:\n",
    "                if a == del_bin:\n",
    "                    vals.remove((a,b))\n",
    "        iter_loop += 1\n",
    "        \n",
    "        # Remove the current max\n",
    "        for i in range(0, len(idx_lst)):\n",
    "            if idx_lst[i] == del_bin or idx_cust[i] == del_clust:\n",
    "                count_vals[i] = -1111\n",
    "\n",
    "    # Assign the unassigned values.\n",
    "    if len(truth_bins) != 0:\n",
    "        for item in range(0, len(truth_bins)):\n",
    "            assigned_bin[truth_bins[item]] = truth_clusts[item]\n",
    "\n",
    "\n",
    "    # Add the new bins to the dataframe.\n",
    "    new_loc_df['new_bins'] = 'na'\n",
    "\n",
    "    # Assign the new bins to the dataframe.\n",
    "    for i in range(0, len(new_loc_df)):\n",
    "        for key, val in assigned_bin.items():\n",
    "            if int(new_loc_df['clust'][i]) == val:\n",
    "                new_loc_df['new_bins'][i] = key    \n",
    "    \n",
    "    return new_loc_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    # Module to read all the csv files\n",
    "    col_names = []\n",
    "    for i in range(1, 32):\n",
    "        col_name_val = 'c' + str(i)\n",
    "        col_names.append(col_name_val)\n",
    "    \n",
    "    #col_names.append('class')\n",
    "    # Read the meal data\n",
    "    meal1 = pd.read_csv('Dataset/mealData1.csv', names = col_names)\n",
    "    meal2 = pd.read_csv('Dataset/mealData2.csv', names = col_names)\n",
    "    meal3 = pd.read_csv('Dataset/mealData3.csv', names = col_names)\n",
    "    meal4 = pd.read_csv('Dataset/mealData4.csv', names = col_names)\n",
    "    meal5 = pd.read_csv('Dataset/mealData5.csv', names = col_names)\n",
    "    meal_df = pd.concat([meal1, meal2, meal3, meal4, meal5], ignore_index = True)\n",
    "    \n",
    "    # Read the meal amount data.\n",
    "    meal_amt1 = pd.read_csv('Dataset/mealAmountData1.csv', names = ['Meal_Amount'])\n",
    "    meal_amt2 = pd.read_csv('Dataset/mealAmountData2.csv', names = ['Meal_Amount'])\n",
    "    meal_amt3 = pd.read_csv('Dataset/mealAmountData3.csv', names = ['Meal_Amount'])\n",
    "    meal_amt4 = pd.read_csv('Dataset/mealAmountData4.csv', names = ['Meal_Amount'])\n",
    "    meal_amt5 = pd.read_csv('Dataset/mealAmountData5.csv', names = ['Meal_Amount'])\n",
    "    \n",
    "    # Concat the cluster values\n",
    "    amount_df = pd.concat([meal_amt1[0 : 51], meal_amt2[0 : 51], meal_amt3[0 : 51], meal_amt4[0 : 51], meal_amt5[0 : 51]], ignore_index = True)\n",
    "    meal_df['clust'] = amount_df\n",
    "    \n",
    "    # Create the feature data frame.\n",
    "    feature_df = pd.DataFrame(columns = ['coeff_0', 'coeff_1', 'coeff_2', 'high_1', 'high_2', 'high_3', 'cgm_velocity_stdv', 'cgm_velocity_mean', 'cgm_velocity_median', 'max_welch', 'std_welch', 'mean_welch', 'median_welch'])\n",
    "    # Interpolate the missing values in\n",
    "    # meal data.\n",
    "    for i in range(len(meal_df)):\n",
    "        interpolate_missing_vals(i, 'meal')\n",
    "    \n",
    "    # Remove all NA values from the dataframe\n",
    "    meal_df = meal_df.dropna()\n",
    "    \n",
    "    # Add Features\n",
    "    new_training = meal_df.loc[:, 'c1' : 'c31'].copy()\n",
    "\n",
    "    # Perform Polyfit\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_polyfit(i)\n",
    "\n",
    "    # Perform polyfit feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_fft(i)\n",
    "    \n",
    "    # Perform CGM velocity feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        cgm_velocity(i)\n",
    "    \n",
    "    # Perform welch feature\n",
    "    for i in range(0, len(new_training)):\n",
    "        perform_welch(i)\n",
    "    \n",
    "    # Perform PCA\n",
    "    final_df = performPCA()\n",
    "    \n",
    "    db_scan_df = final_df.copy()\n",
    "    k_means_df = final_df.copy()\n",
    "    \n",
    "    # Perform dbscan\n",
    "    db_scan()\n",
    "    \n",
    "    # Perform kmeans\n",
    "    perform_kmns()\n",
    "    \n",
    "    # Add ground truth\n",
    "    db_scan_df['true_amt'] = amount_df\n",
    "    db_scan_df['bins'] = 'NA'\n",
    "    # Add ground truth\n",
    "    k_means_df['true_amt'] = amount_df\n",
    "    k_means_df['bins'] = 'NA'\n",
    "    \n",
    "    db_scan_df = add_bins(db_scan_df)\n",
    "    k_means_df = add_bins(k_means_df)\n",
    "    \n",
    "    # Add new truth labels.\n",
    "    db_scan_df = add_new_truth(db_scan_df)\n",
    "    k_means_df = add_new_truth(k_means_df)\n",
    "    # K means trainer\n",
    "    final_tr_km_df = k_means_df.loc[:, 'component_1' : 'component_10']\n",
    "    final_tr_km_labels_df = k_means_df['new_bins'].values\n",
    "    \n",
    "    #create new a knn model\n",
    "    knn = KNeighborsClassifier()\n",
    "    #create a dictionary of all values we want to test for n_neighbors\n",
    "    param_grid = {'n_neighbors': np.arange(1, 25)}\n",
    "    #use gridsearch to test all values for n_neighbors\n",
    "    knn_gscv = GridSearchCV(knn, param_grid, cv=5)\n",
    "    #fit model to data\n",
    "    final_fit = knn_gscv.fit(final_tr_km_df, final_tr_km_labels_df)\n",
    "    pickle.dump(final_fit, open('pkl_files/kmean_fit.pkl', 'wb'))\n",
    "    \n",
    "    # DBSCAN trainer\n",
    "    final_tr_db_df = db_scan_df.loc[:, 'component_1' : 'component_10']\n",
    "    final_tr_db_labels_df = db_scan_df['new_bins'].values\n",
    "    \n",
    "    #create new a knn model\n",
    "    knn_db = KNeighborsClassifier()\n",
    "    #create a dictionary of all values we want to test for n_neighbors\n",
    "    param_grid = {'n_neighbors': np.arange(1, 25)}\n",
    "    #use gridsearch to test all values for n_neighbors\n",
    "    knn_db_gscv = GridSearchCV(knn_db, param_grid, cv=5)\n",
    "    #fit model to data\n",
    "    final_fit_db = knn_db_gscv.fit(final_tr_db_df, final_tr_db_labels_df)\n",
    "    pickle.dump(final_fit_db, open('pkl_files/dbscan_fit.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
